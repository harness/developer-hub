import createAlertPolicy from '@site/docs/feature-management-experimentation/50-release-monitoring/static/metrics/create-alert-policy.png';
import alertPolicy from '@site/docs/feature-management-experimentation/50-release-monitoring/static/metrics/create-new-policy.png';
import metricAlert from '@site/docs/feature-management-experimentation/50-release-monitoring/static/metrics/generate-results-button.png';

## Overview

:::info Get access to alert policies
To enable alert policies for your account, contact your customer success manager or [Harness Support](/docs/feature-management-experimentation/fme-support).
:::

A metric alert policy allows you to configure a degradation threshold for your metrics. Split monitors your metric impact for every feature flag using a percentage rollout for the duration of the monitor window. 

You will be alerted if the observed impact for a metric exceeds the threshold you define.

## Creating a metric alert policy

To create a metric alert policy, do the following:

1. In the Split UI, create a new metric or click on a metric for which you want to create an alert policy. For more about creating a metric, see [Create a metric](/docs/feature-management-experimentation/release-monitoring/metrics/setup/#create-a-metric).

1. On the metric’s Alert policy tab, click the **Create alert policy** button.
   
   <img src={createAlertPolicy} alt="Create Alert Policy" width="700" />

1. Provide a name and description for the alert policy.

   <img src={alertPolicy} alt="Alert Policy Example" width="700" />

   * **Name**: Give your alert policy a human recognizable name. We recommend also including the metric name your alert policy is associated with in your policy name.
   * **Description**: Optionally, give your alert policy a description. This can include anything that would be useful to you and your team if an alert is fired, such as runbooks, alerting protocols, and key information about the alert.

1. An alert policy can have multiple alert conditions. For each alert condition, fill in the fields as follows:

   * **Choose your environment**: Select the environment you want to apply the alert condition to. Each alert condition relates to a particular environment, and you are limited to one alert condition per environment. When you create additional alert conditions, any environments for which you have already configured an alert condition will not be available in the environment menu list.
   * **Set alert threshold**: You can add a relative percentage threshold or absolute value threshold. An alert will fire when this threshold is exceeded in the degradation direction (the opposite of the desired direction of the metric). See the [Choosing your degradation threshold for alerting](/docs/feature-management-experimentation/release-monitoring/alerts/alert-policies/monitoring-window#choosing-your-degradation-threshold-for-alerting) guide for more information.
   
      :::note 
      If you set a degradation threshold of 0, Split will notify you of observed impact in both the desired and undesired directions.
      :::

   * **Define alert notification channel**: Configure who to notify if an alert is fired. By default, the metric owner is selected. You can also select the feature flag owners, any existing users and groups, and type in additional emails of your choice.

1. Click the **Add alert condition** button to add additional alert conditions for other environments.

1. After you finish entering your conditions, click the **Create alert policy** button. You now have a new alert policy.

   :::info
   Alert policies can only be created for metrics that are measured per traffic type, rather than those that are measured across all traffic types keys.
   :::

## When a metric alert is triggered

When Split identifies that a feature flag has caused a degradation in a metric, an alert is triggered for the metric alert policy, and the recipients you selected in the alert notification channel will receive an email. The alert summary will also be displayed on the Definition and Alerts tabs for the feature flag that caused the degradation.

When a metric alert is triggered, the recipients will receive an email in the following format:

```
Subject: "Split Alert Fired: Feature Flag Name has caused Metric Name to degrade by X%"
Body: "Split has detected a degradation of X% on your Metric Name Metric. Here are the details:

Feature flag: (name and link to feature flag)
Alert Policy: (name and link to alert policy)
Metric: (name and link to metric)
Baseline Value:
Treatment Value:
Impact %:
Threshold %:
Please either kill the feature flag or dismiss this alert in the Split user interface.
-The Split Team

You are receiving this email because you were set as one of the people to be notified on this Alert Policy: (name and link to Alert Policy). If you don't want to receive these emails, please remove yourself from the list of people to be notified on the Alert policy page."
```

The alert notification email contains information about the specific feature flag version, rule, and treatment (in comparison with the [baseline treatment](/docs/feature-management-experimentation/feature-management/set-the-alert-baseline-treatment/)) that triggered the alert. In the Split UI, you can explore the results further on the Metrics impact page.

## Metric alert summary in Split UI

When a feature flag causes a metric alert to trigger, a banner will be displayed on the feature flag’s Definition tab. Additional details will also be available on the feature flag’s Alerts tab.

<img src={metricAlert} alt="Metric Alert" width="700" />

If an alert triggers, the following information is displayed in a table within the Alerts tab:

| Column header | Description |
|---|---|
| Alert fired | Displays the timestamp when the metric alert is triggered. You can take action on the alert by clicking on the menu list. |
| Activity | Displays the status of the alert, whether it is dismissed or auto-resolved (learn more below). Hover over the text to access the timestamp of the action in this column. |
| Name of policy | Displays the name of the metric alert policy. Click the name to go directly to the alert policy builder. |
| Rule | Displays the feature flag's targeting rule that caused the alert to fire. |
| Relative impact % | Displays the degradation percentage that is detected and the error margin of this percentage. Hover over the text to access the timestamp of when the degradation is detected. |
| Absolute impact | Displays the degradation that is detected in an absolute value and the error margin of this value. Hover over the text to access the timestamp of when the degradation is detected. |
| Metric value (baseline) | Displays the metric value in absolute terms and the baseline treatment that is used for alert monitoring. |
| Metric value (treatment) | Displays the metric value in absolute terms as well as the treatment that is used to compare against the baseline treatment for alert monitoring. |
| Relative threshold % | Displays the degradation threshold configured in the metric alert policy definition in relative terms or a translation of the absolute threshold if this is used in the alert policy definition. Hover over the text to see which threshold type is used in the alert policy definition. |
| Absolute threshold | Provides the degradation threshold configured in the alert policy definition in absolute terms or a translation of the relative threshold if this is used in the alert policy definition. Hover over the text to see which threshold type is used in the alert policy definition. |

For each alert, you can take the following actions:

| Action | Description |
|---|---|
| Kill feature flag | If you decide to kill a feature flag due to an alert, the default treatment overrides the existing targeting rules and is returned for all customers. |

## Troubleshooting metric alerts

For troubleshooting an alert that did not fire as expected, refer to the [Troubleshooting alerts](/docs/feature-management-experimentation/release-monitoring/alerts/troubleshooting) guide.