The Feature Management & Experimentation(FME) Developer exam tests your knowledge and skills of the Harness Feature Management & Experimentation module.  

## Prerequisites

- Basic terminal skills
- Basic understanding of on-premise or cloud architecture

## Exam Details
| Exam Type                               | Duration         |
| ----------------------------------- | --------------- |
| **Knowledge Exam** | 90 minutes |

| Covered Domain                                   | Coverage |
|------------------------------------------------|----------|
| Release Monitoring & Impact Detection           | 12%      |
| Metrics, Impressions & Attribution              | 15%      |
| Feature Flags, SDKs & Performance               | 13%      |
| Experiment Design & Statistical Analysis        | 15%      |
| Metrics Design & Measurement Strategy           | 15%      |
| Alerting & Troubleshooting                      | 15%      |
| Attribution, Rule Changes & Exclusions          | 8%       |
| Platform Navigation, Governance & RBAC          | 7%       |


<br />

## Exam Objectives

<details>
<summary>List of Objectives</summary>

The following is a detailed list of exam objectives:

| #   | Objective |
|-----|-----------|
| 1   | **Release Monitoring & Impact Detection** |
| 1.1 | Explain the purpose of Release Monitoring in Harness FME. |
| 1.2 | Describe why percentage-based rollouts are required for statistically valid release monitoring. |
| 1.3 | Identify how configuration and monitoring choices affect release monitoring outcomes. |
| 2   | **Metrics, Impressions & Attribution** |
| 2.1 | Explain how impressions and events work together to calculate metrics. |
| 2.2 | Determine when impressions are generated and how impression tracking affects metric visibility. |
| 2.3 | Diagnose issues caused by disabling impression tracking or unsupported SDK behavior. |
| 3   | **Feature Flags, SDKs & Performance** |
| 3.1 | Apply best practices for organizing feature flags to optimize SDK performance. |
| 3.2 | Explain the role of impression metadata and targeting rule labels in debugging. |
| 3.3 | Differentiate control treatments, fallback treatments, and evaluation failure behavior. |
| 4   | **Experiment Design & Statistical Analysis** |
| 4.1 | Identify the correct lifecycle and phases of experimentation. |
| 4.2 | Explain the importance of hypothesis-driven experimentation and goal alignment. |
| 4.3 | Evaluate statistical tradeoffs in experiment analysis. |
| 5   | **Metrics Design & Measurement Strategy** |
| 5.1 | Select appropriate metric types based on experiment goals. |
| 5.2 | Configure metrics correctly to measure performance changes. |
| 5.3 | Explain how key metrics enable alerting and monitoring. |
| 6   | **Alerting & Troubleshooting** |
| 6.1 | Identify prerequisites required for metric alert policies to fire. |
| 6.2 | Diagnose why metric or significance alerts fail to fire. |
| 6.3 | Explain alert lifecycle behavior, including auto-resolution. |
| 7   | **Attribution, Rule Changes & Exclusions** |
| 7.1 | Explain how event timing affects metric attribution. |
| 7.2 | Analyze attribution behavior when targeting rules change. |
| 7.3 | Identify when user data is excluded from metric calculations. |
| 8   | **Platform Navigation, Governance & RBAC** |
| 8.1 | Describe core Harness FME platform concepts and structure. |
| 8.2 | Navigate operational dashboards and customer-level data. |
| 8.3 | Apply governance rules for users, groups, naming, and traffic types. |


</details>

<br />

## Next Steps

The Feature Management & Experimentation Developer exam can start immediately after registering. Please allow up to 90 mins to complete the knowledge exam.

1. Create an account in Harness University
2. Review the Study Guide above. 
3. Register for an exam. 
4. Take the exam.
